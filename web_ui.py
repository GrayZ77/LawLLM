import json
import match
from io import StringIO
import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
from streamlit_extras.mention import mention


st.set_page_config(page_title="DISC-LawLLM")
st.title("ğŸ¦œLawLLM-With-LangChain")
st.caption("ğŸš€ A streamlit chatbot powered by FudanDISC-LLM, running on Ubuntu-22.04")

"""
è¯¥é—®ç­”ç³»ç»Ÿä»¥`LangChain`ä¸ºåŸºæœ¬æ¡†æ¶ï¼Œå®Œæˆäº†å‘é‡æ•°æ®åº“çš„æ„å»ºä¸æ–‡æœ¬æ£€ç´¢ã€‚æ›´å¤šæœ‰å…³`LangChain`çš„å†…å®¹è¯·è®¿é—®[langchain-io.com](https://www.langchain-io.com/)
"""

@st.cache_resource()
def init_model():
    model_path = "/root/DISC-LawLLM/model"
    model = AutoModelForCausalLM.from_pretrained(
        model_path, torch_dtype=torch.float16, device_map="auto", trust_remote_code=True, local_files_only = True, offload_folder = "offload"
    )
    model.generation_config = GenerationConfig.from_pretrained(model_path, local_files_only = True)
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, use_fast=False, trust_remote_code=True, local_files_only = True
    )
    return model, tokenizer


def clear_chat_history():
    del st.session_state.messages
    del st.session_state.dialogs


def init_chat_history():
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯ DISC-LawLLMï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ’–")

   

    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "dialogs" not in st.session_state:
        st.session_state.dialogs = []

    else: 
        for message in st.session_state.dialogs:
            avatar = "ğŸ™‹â€â™‚ï¸" if message["role"] == "user" else "ğŸ¤–"
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
                if message["role"] == "assistant" and "result" in message:
                    with st.expander("æŸ¥çœ‹æ£€ç´¢ç»“æœ", expanded=False):
                        st.markdown(message["result"])
                

    return st.session_state.messages, st.session_state.dialogs


def main():
    model, tokenizer = init_model()
    messages, dialogs = init_chat_history()
    with st.sidebar:
        mention(
            label = "Source Code",
            icon = "github",
            url = "https://github.com/GrayZ77/LawLLM",
        )
    
    
    _stream = st.sidebar.checkbox("å¼€å¯æµå¼è¾“å‡º", value=True)
    _match = st.sidebar.checkbox("å¼€å¯æ³•æ¡æ£€ç´¢")
           

    if _match:
        _num = st.sidebar.slider("è¯·é€‰æ‹©æ£€ç´¢æ³•æ¡æ•°", 1, 5, 3)
        if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
            with st.chat_message("user", avatar="ğŸ™‹â€â™‚ï¸"):
                st.markdown(prompt)
            result = match.quest(prompt, num=_num)
            question = f"ä»¥è‡ªå·±çš„æ€è€ƒä¸ºä¸»ï¼Œå‚è€ƒç»™å‡ºçš„å†…å®¹ï¼Œå›ç­”ä¸‹é¢çš„é—®é¢˜ã€‚å¦‚æœç»™å‡ºçš„å†…å®¹æ²¡æœ‰å‚è€ƒä»·å€¼ï¼Œå°±å¿½ç•¥æ‰è¿™äº›å†…å®¹ã€‚\n\nå‚è€ƒï¼š{result}\n\né—®é¢˜ï¼š{prompt}"
            dialogs.append({"role": "user", "content": prompt})
            messages.append({"role": "user", "content": question})
            print(f"[user] {question}", flush=True)
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                placeholder = st.empty()
                with st.expander("æŸ¥çœ‹æ£€ç´¢ç»“æœ", expanded=False):
                    st.markdown(result)
                for response in model.chat(tokenizer, messages, stream=_stream):
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
            messages.append({"role": "assistant", "content": response})
            dialogs.append({"role": "assistant", "content": response, "result": result})
            print(json.dumps(messages, ensure_ascii=False), flush=True)
    
    else:
        if prompt := st.chat_input("Shift + Enter æ¢è¡Œï¼ŒEnter å‘é€"):
            with st.chat_message("user", avatar="ğŸ™‹â€â™‚ï¸"):
                st.markdown(prompt)
            dialogs.append({"role": "user", "content": prompt})
            messages.append({"role": "user", "content": prompt})
            print(f"[user] {prompt}", flush=True)
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                placeholder = st.empty()
                for response in model.chat(tokenizer, messages, stream=_stream):
                    placeholder.markdown(response)
                    if torch.backends.mps.is_available():
                        torch.mps.empty_cache()
            messages.append({"role": "assistant", "content": response})
            dialogs.append({"role": "assistant", "content": response})
            print(json.dumps(messages, ensure_ascii=False), flush=True)

    st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)



if __name__ == "__main__":
    main()
